# coding: utf-8
"""
March 2021
inspiration: copy paste from generate_cci_sea_state_daily_nc_file.py but moidification SAFE granularity plus DLR format proposition
A Grouazel
tested/validated in env /home1/datawork/agrouaze/conda_envs2/envs/cwave/bin/python
example usage:
export PYTHONPATH=/home1/datahome/agrouaze/git/osm_landmask_distancecoast
export PYTHONPATH=/home1/datahome/agrouaze/git/swiml2sproc/:$PYTHONPATH
export PYTHONPATH=/home1/datahome/agrouaze/git/mpc/data_collect/:$PYTHONPATH
export PYTHONPATH=/home1/datahome/agrouaze/git/sar_hs_nn:$PYTHONPATH
export PYTHONPATH=/home1/datahome/agrouaze/git/xsarseafork/src/xsarsea:$PYTHONPATH
python /home1/datahome/agrouaze/git/sar_hs_nn/sarhspredictor/lib/generate_cci_sea_state_orbit_nc_file.py --outputdir /tmp/ --redo --cwave-version v3.1 --dev --verbose --safename /home/datawork-cersat-public/project/mpc-sentinel1/data/esa/sentinel-1a/L2/WV/S1A_WV_OCN__2S/2021/001/S1A_WV_OCN__2SSV_20210101T053947_20210101T060035_035940_0435AC_E7DD.SAFE/
pour lancement sur datarmor voir exe_hs_total_empirical_cwave_v3.1_CCI.pbs
"""

import logging
import sys
import xarray
import os
import time
import glob
import netCDF4
import pandas as pd
import traceback
os.environ["KMP_WARNINGS"] = "FALSE"
import datetime
import collections
import resource
import numpy as np
import uuid
import pdb
from shared_information import PROJECT_DIR_DATARMOR,sats_acro,DIR_HS_EMP_CWAVE_CCI,VERSION_CWAVE_CCI
from sarhspredictor.lib.predict_with_quach2020_on_OCN_using_keras import main_level_1
from sarhspredictor.lib.load_quach_2020_keras_model import load_quach2020_model_v2,load_quach2020_model_exp1
from sarhspredictor.config import land_polygon_path,land_raster_path
from sarhspredictor.lib.predict_hs_from_SLC import predictions_from_slc_SAFE
from read_era05_meteo_params import get_params_at_locations
from add_bathymetry_from_GEBCO import GEBCO14Bathymetry

from coastaux.lib import swiml2sproc_utils_ancillary_landmask

from swiml2sproc.utils.ancillary.seaice import get_seaice_concentration


from get_path_from_base_SAFE import get_path_from_base_SAFE # mpc/data_collect
import match_SAFE_L1_L2 # mpc/data_collect
from get_classification_infos import get_classification_infos_safe_granularity #,get_classification_infos #Mpc/qualitycheck
from match_measu_wv_with_classif_xml import get_xml_path_from_safe  #mpc/qualitycheck
from reader_classif_wv_cls_xml import read_xml_classif_ID #mpc/qualitycheck
INPUT_TXT_DIR = '/home1/scratch/agrouaze/sentinel1/L2/WV/hs_total_SAR/v1/'
INPUT_TXT_DIR = '/home1/datahome/agrouaze/sentinel1/L2/WV/hs_total_SAR/v1/'
INPUT_TXT_DIR = '/home1/datawork/agrouaze/sentinel1/L2/WV/hs_total_SAR/v1/'
OUTDIR = '/home/cercache/users/agrouaze/temporaire/sentinel1/hs_total_SAR/hs_total_sar_v1_from_L2/'  # first directory for the version with some Hs total SAR bugged due to the mkl lib env issue
OUTDIR = '/home/datawork-cersat-public/project/cci-seastate/sandbox/data/sar/'  # path given by JFP in december2017
OUTDIR = '/home/datawork-cersat-public/project/cci-seastate/sandbox/data/sar/v1.0/'  # contained Hs total only separated by satellite
OUTDIR = '/home/datawork-cersat-public/project/cci-seastate/sandbox/data/sar/v1.1/'  # .nc separated by incidence angle and satellite
OUTDIR = DIR_HS_EMP_CWAVE_CCI

FILE_VERSION = '01'
antarctic_seaice_path="/home/ref-cersat-public/sea-ice/sea-ice-concentration/antarctic/ssmi/CER_PSI_ANT_1D_012_PSI_SS/daily/netcdf/%Y/"
arctic_seaice_path="/home/ref-cersat-public/sea-ice/sea-ice-concentration/arctic/ssmi/CER_PSI_ARC_1D_012_PSI_SS/daily/netcdf/%Y/"

time.sleep(np.random.randint(0,10,1)[0]) # to avoid collision when creatting mkdir of output files for parallel processing
#OUTDIR = '/home1/scratch/agrouaze/'
version_hs_computation = 'v1'  # orignal version reading the txt generated by PRUN
version_hs_computation = 'v1.1'  # new version that compute directly from netCDF with a corrected mkl lib in environement but still using the logpolar->cartesian conversion (January 2019)
version_hs_computation = VERSION_CWAVE_CCI
version_hs_computation = 'v3' # Quach 2020
version_hs_computation = 'v3.1' #modification to have files nc per SAFE
version_hs_computation = 'v3.2' #rejection flags based on tenGEOP classification
units_dates = 'seconds since 2014-04-06 00:00:00'
units_dates = 'seconds since 1990-01-01'
# key: values (standard_name ,type,dimensions,units,descr/long_name,vmin,vmax,standard_name)
variables_infos = {  # ('time','str3')
    #"big_sat_acro" : ("satellite_name",'S3',('time',),'no units','first 3 characters of satellite name',None,None),
    #'big_kcorrup' : ("flag_k_vector_corrupted",'f4',('time',),'no units',
    #                 'flag to indicate if the k vector of original ESA product is corrupted (True) or not (False)',None,
    #                 None),
    'swh' : ("sea_surface_wave_significant_height",'f8',('time',),'m','C band significant wave height',0.,30.,''),
    'swh_uncertainty' : (
                        'swh_uncertainty','f8',('time',),'m','standard deviation associated to hs :  level of confidence of the NN model ',0.,6.,''),
    'swh_quality':("swh_quality","byte",('time'),"","quality of C band significant wave height measurement","","",''),
    'swh_rejection_flags':('swh_rejection_flags','byte',('time'),'','consolidated instrument and ice flags',"","",''),
    'angle_of_incidence' : ("angle_of_incidence",'f8',('time',),'degree','incidence angle of the WV acquisition',22.,38.,'angle_of_incidence'),
    #'oswLandFlag':('land_flag','byte',('time'),'','land flag annotated in ESA OCN WV products',0,1),
    'distance_to_coast':('distance_to_coast','f8',('time'),'m','distance to nearest coast for WV image center',0.,4000000.),
    #'platformName':('platform_name','S3',('time'),'','name of the satellite','',''),
    'lon':('longitude','f8',('time'),'degrees_east','longitude',-180.,180.),
    'lat':('latitude','f8',('time'),'degrees_north','latitude',-90.,90.),
    'sigma0' : ("sigma0",'f8',('time',),'dB','sigma0 (Normalized Radar Cross Section)',-30.,10.),
    'normalized_variance' : ("normalized_variance",'f8',('time',),'','Normalized variance of digital numbers (complex I+Q values)',1.,3.),
    'wind_speed' : ("wind_speed",'f8',('time',),'m s-1','wind speed coming from ESA OCN WV product (CMOD-based wind inversion without Bayesian scheme)',0.,50.),
    'heading' : ("satellite_heading",'f8',('time',),'degree',
                     'satellite heading relative to geographic North in clockwise convention',0.,360.),
    'sea_ice_fraction':('sea ice fraction','f8',('time'),'1','',0.,1.,'sea_ice_fraction'),
    'time':('time','f8',('time'),units_dates,''),
    'wind_speed_model_u':('wind_speed_model_u','f8',('time',),'m s-1','10 metre U wind component',0.,40.,'eastward_wind'),
    'wind_speed_model_v':('wind_speed_model_v','f8',('time',),'m s-1','10 metre V wind component',0.,40.,'northward_wind'),
    'surface_air_pressure':('surface_air_pressure','f8',('time',),'Pa','Mean sea level pressure',"","",'air_pressure_at_sea_level'),
    'surface_air_temperature':('surface_air_temperature','f8',('time',),'K','2 metre temperature',"","",'air_temperature'),
    'swh_model':('sea_surface_wave_significant_height_ERA5','f8',('time',),'m','significant wave height from ERA5 0.50deg resolution',"","",''),
    'bathymetry':('sea_floor_depth_below_mean_sea_level','f8',('time',),'m','ocean depth',"","",'sea_floor_depth_below_mean_sea_level'),
   # 'sst':('surface_air_pressure','f8',('time',),'Pa','M',"","",'air_temperature'),
}


def add_classification_proba(basesafe,measu_list):
    meta_classif = {}
    df_classif = None

    path_pot_xml = get_xml_path_from_safe(basesafe)
    #

    if path_pot_xml is not None :
        if os.stat(path_pot_xml).st_size == 0 :
            logging.info('%s is empty',path_pot_xml)  # it happends that DL_Summary.xml files are empty
        else :
            df_classif = read_xml_classif_ID(path_pot_xml)
    if df_classif is not None :
        for ii in measu_list :
            content = {}
            errors = collections.defaultdict(int)
            counters = collections.defaultdict(int)
            #content,errors,counters = get_classification_infos(ii,content,errors,counters)
            content,errors,counters = get_classification_infos_safe_granularity(measu=ii,dataFrom_xml=df_classif,content=content,errors=errors
                                                      ,counters=counters)
            for cla in ['SI','IB','RC','BS','LWA'] :
                if cla not in meta_classif :
                    meta_classif[cla] = [content[cla]]
                else :
                    meta_classif[cla].append(content[cla])
    else :  # si pas de classification je met tout a zero
        for cla in ['SI','IB','RC','BS','LWA'] :
            for ii in measu_list :
                if cla not in meta_classif :
                    meta_classif[cla] = [0.]
                else :
                    meta_classif[cla].append(0.)
    return meta_classif

def read_infos_from_WV_ifremer_archive_v3 ( safename,model,model_version,dev=False ) :
    """
    read OCN L2 WV data and compute on the fly the Hs predicted by NN model
    :args:
        safename (str): basename L2 WV SAFE
        model (Keras obj):
        model_version (str): for instance exp1
    """
    logging.info('safename : %s',safename)
    measu_list = []
    sato = safename[0:3]
    #for ss in [sato] :
    fullsafepath = get_path_from_base_SAFE(safename)
    measu_listtmp = glob.glob(os.path.join(fullsafepath,'measurement','*nc'))
    # filter on incidence angle
    #measu_listtmp2 = [ffh for ffh in measu_listtmp if wv in ffh]
    measu_listtmp2 = measu_listtmp # we want wv1 and wv2 together in the same nc files at the end
    measu_list += sorted(measu_listtmp2)
    if dev:
        logging.warning('dev mode reduction of input listing to first 6 ocn')
        measu_list = measu_list[0:6]
    # add classification tengeoP roughness
    meta_classif = add_classification_proba(safename,measu_list)


    logging.info('Nb ocn files to read: %s',len(measu_list))
    if len(measu_list)>0:
        if model_version =='exp1':
            logging.info('start exp1 inferences')
            full_path_safe_SLC = match_SAFE_L1_L2.getL1SAFEcorrespondingToL2SAFE(fullsafepath,L1_procesing_type='SLC')
            logging.info('full_path_safe_SLC : %s',full_path_safe_SLC)
            s1_wv_ds = predictions_from_slc_SAFE(safe_path=full_path_safe_SLC,model=model,dev=dev)
        else:
            s1_wv_ds = main_level_1(measu_list[: :-1],model)
        #s1_wv_ds = replace_osw_position_by_rvl(s1_wv_ds)
        for uu in meta_classif:
            s1_wv_ds[uu] = xarray.DataArray(np.array(meta_classif[uu]),dims=['time'])
    else:
        s1_wv_ds = None
    return s1_wv_ds


def apply_sea_ice_mask(ds,seaice_conc):
    """

    :param ds:
    :param seaice_conc : float between 0 and 100
    :return:
    """
    #TBD

    seaice_max = 10.
    ind_seaice_detected = seaice_conc>seaice_max
    qualityvals = ds['swh_quality'].values
    swh_rejection_flags = ds['swh_rejection_flags'].values
    qualityvals[ind_seaice_detected] = 1  # bad
    #swh_rejection_flags[ind_seaice_detected] += np.int(2 ** 128)  # not_water
    aa = np.zeros(len(swh_rejection_flags)).astype(int)
    swh_rejection_flags[ind_seaice_detected] += aa[ind_seaice_detected] | (1 << 4)
    ds['swh_quality'] = xarray.DataArray(qualityvals,dims='time')
    ds['swh_rejection_flags'] = xarray.DataArray(swh_rejection_flags,dims='time')
    return ds

def apply_sea_ice_mask_classif(ds):
    ind_sea_ice_classif = ds['SI']>0.9
    qualityvals = ds['swh_quality'].values
    swh_rejection_flags = ds['swh_rejection_flags'].values
    qualityvals[ind_sea_ice_classif] = 1  # bad
    # swh_rejection_flags[ind_seaice_detected] += np.int(2 ** 128)  # not_water
    aa = np.zeros(len(swh_rejection_flags)).astype(int)
    swh_rejection_flags[ind_sea_ice_classif] += aa[ind_sea_ice_classif] | (1 << 5)
    ds['swh_quality'] = xarray.DataArray(qualityvals,dims='time')
    ds['swh_rejection_flags'] = xarray.DataArray(swh_rejection_flags,dims='time')
    return ds

def apply_biologicalslicks_mask_classif(ds):
    ind_sea_ice_classif = ds['BS']>0.9
    qualityvals = ds['swh_quality'].values
    swh_rejection_flags = ds['swh_rejection_flags'].values
    qualityvals[ind_sea_ice_classif] =1  # bad
    # swh_rejection_flags[ind_seaice_detected] += np.int(2 ** 128)  # not_water
    aa = np.zeros(len(swh_rejection_flags)).astype(int)
    swh_rejection_flags[ind_sea_ice_classif] += aa[ind_sea_ice_classif] | (1 << 6)
    ds['swh_quality'] = xarray.DataArray(qualityvals,dims='time')
    ds['swh_rejection_flags'] = xarray.DataArray(swh_rejection_flags,dims='time')
    return ds

def apply_icerberg_mask_classif(ds):
    ind_sea_ice_classif = ds['IB']>0.9
    qualityvals = ds['swh_quality'].values
    swh_rejection_flags = ds['swh_rejection_flags'].values
    qualityvals[ind_sea_ice_classif] = 1  # bad
    # swh_rejection_flags[ind_seaice_detected] += np.int(2 ** 128)  # not_water
    aa = np.zeros(len(swh_rejection_flags)).astype(int)
    swh_rejection_flags[ind_sea_ice_classif] += aa[ind_sea_ice_classif] | (1 << 7)
    ds['swh_quality'] = xarray.DataArray(qualityvals,dims='time')
    ds['swh_rejection_flags'] = xarray.DataArray(swh_rejection_flags,dims='time')
    return ds

def apply_raincell_mask_classif(ds):
    ind_sea_ice_classif = ds['RC']>0.9
    qualityvals = ds['swh_quality'].values
    swh_rejection_flags = ds['swh_rejection_flags'].values
    qualityvals[ind_sea_ice_classif] = 1  # bad
    # swh_rejection_flags[ind_seaice_detected] += np.int(2 ** 128)  # not_water
    aa = np.zeros(len(swh_rejection_flags)).astype(int)
    swh_rejection_flags[ind_sea_ice_classif] += aa[ind_sea_ice_classif] | (1 << 8)
    ds['swh_quality'] = xarray.DataArray(qualityvals,dims='time')
    ds['swh_rejection_flags'] = xarray.DataArray(swh_rejection_flags,dims='time')
    return ds

def apply_lwa_mask_classif(ds):
    ind_sea_ice_classif = ds['LWA']>0.9
    qualityvals = ds['swh_quality'].values
    swh_rejection_flags = ds['swh_rejection_flags'].values
    qualityvals[ind_sea_ice_classif] = 1  # bad
    # swh_rejection_flags[ind_seaice_detected] += np.int(2 ** 128)  # not_water
    aa = np.zeros(len(swh_rejection_flags)).astype(int)
    swh_rejection_flags[ind_sea_ice_classif] += aa[ind_sea_ice_classif] | (1 << 9)
    ds['swh_quality'] = xarray.DataArray(qualityvals,dims='time')
    ds['swh_rejection_flags'] = xarray.DataArray(swh_rejection_flags,dims='time')
    return ds

def apply_landmask_in_rejection_flag(ds):

    indexon_on_land = (ds['oswLandFlag'].values==1)
    qualityvals = ds['swh_quality'].values
    swh_rejection_flags = ds['swh_rejection_flags'].values
    qualityvals[indexon_on_land ] = 1 #bad
    #swh_rejection_flags[indexon_on_land] += np.int(2 ** 128) #not_water
    aa = np.zeros(len(swh_rejection_flags)).astype(int)
    swh_rejection_flags[indexon_on_land] += aa[indexon_on_land] | (1 << 4)
    ds['swh_quality'] = xarray.DataArray(qualityvals,dims='time')
    ds['swh_rejection_flags'] = xarray.DataArray(swh_rejection_flags,dims='time')
    return ds


def apply_windspeed_mask(ds):
    """

    :param ds:
    :return:
    """
    ind_below_2ms  = (ds['oswWindSpeed'].values<2)
    qualityvals = ds['swh_quality'].values
    swh_rejection_flags = ds['swh_rejection_flags'].values
    qualityvals[ind_below_2ms & (qualityvals>2)] = 2 #acceptable
    #swh_rejection_flags[ind_below_2ms] += np.int(2 ** 64)  # wind_below_2_m_per_sec
    aa = np.zeros(len(swh_rejection_flags)).astype(int)
    swh_rejection_flags[ind_below_2ms] += aa[ind_below_2ms] | (1 << 3)
    # je devrais faire comme ca pour les bytes flag_per += seaice_mask * np.int8(2 ** 2) (vu dans proc swiml2s)
    ds['swh_quality'] = xarray.DataArray(qualityvals,dims='time')
    ds['swh_rejection_flags'] = xarray.DataArray(swh_rejection_flags,dims='time')
    return ds

def apply_nv_mask(ds):
    """

    :param ds:
    :return:
    """
    max_nv_wv1 = 1.85 # a discuter avec Justin et Alexis valeur utilisee pour Maupiti project
    max_nv_wv2 = 1.3
    logging.debug('Nv values: %s',ds['nv'].values) #here nv at not been rename yet
    indwv1 = (ds['oswIncidenceAngle'].values<30)
    indwv2 = (ds['oswIncidenceAngle'].values>30)
    #if wv == 'wv1':
    ind_NVgreater1 = (ds['nv'].values > max_nv_wv1) & indwv1
    #else:
    ind_NVgreater2 = (ds['nv'].values > max_nv_wv2) & indwv2
    ind_NVgreater = ind_NVgreater1 | ind_NVgreater2
    qualityvals = ds['swh_quality'].values
    logging.info('%s observations with Nv flag raised ',ind_NVgreater.sum())
    swh_rejection_flags = ds['swh_rejection_flags'].values
    qualityvals[ind_NVgreater  & (qualityvals>2)] = 2 #
    #swh_rejection_flags[ind_NVgreater] = 1  # nv_greater_than_max_nv
    #swh_rejection_flags[ind_NVgreater] += np.int(2 ** 1)
    aa = np.zeros(len(swh_rejection_flags)).astype(int)
    swh_rejection_flags[ind_NVgreater] += aa[ind_NVgreater] | (1 << 0)
    ds['swh_quality'] = xarray.DataArray(qualityvals,dims='time')
    ds['swh_rejection_flags'] = xarray.DataArray(swh_rejection_flags,dims='time')
    return ds

def apply_outlier_swh_based_on_uncertainty(ds):
    """

    :param ds:
    :return:
    """
    one_std = np.mean(ds['swh_uncertainty'].values) #it is a mean because swh_uncertainty is already an STD
    ind_outlier = (ds['swh_uncertainty'].values >= 2*one_std)
    qualityvals = ds['swh_quality'].values
    swh_rejection_flags = ds['swh_rejection_flags'].values
    logging.debug('uncertainty : %s',ds['swh_uncertainty'].values)
    logging.debug('limit outlier: %s',2*one_std)
    qualityvals[ind_outlier ] = 1 #bad , even 0 undefined can be tunred into bad
    logging.debug('%s observations with outlier flag raised ',ind_outlier.sum())
    #swh_rejection_flags[ind_outlier] += np.int(2 ** 2)  # swh_outlier
    aa = np.zeros(len(swh_rejection_flags)).astype(int)
    swh_rejection_flags[ind_outlier] += aa[ind_outlier] | (1 << 1)
    ds['swh_quality'] = xarray.DataArray(qualityvals,dims='time')
    ds['swh_rejection_flags'] = xarray.DataArray(swh_rejection_flags,dims='time')
    return ds


def apply_nan_mask(ds):
    """

    :param ds:
    :return:
    """
    ind_not_finite = (np.isfinite(ds['swh_uncertainty'].values)==False) | (np.isfinite(ds['swh'].values)==False)
    qualityvals = ds['swh_quality'].values
    swh_rejection_flags = ds['swh_rejection_flags'].values
    qualityvals[ind_not_finite ] = 1 #bad , even 0 undefined can be tunred into bad
    logging.debug('%s observations not finite ',ind_not_finite.sum())
    #swh_rejection_flags[ind_not_finite] += np.int(2 ** 4)  # swh_outlier
    aa = np.zeros(len(swh_rejection_flags)).astype(int)
    swh_rejection_flags[ind_not_finite] += aa[ind_not_finite] | (1 << 2)
    ds['swh_quality'] = xarray.DataArray(qualityvals,dims='time')
    ds['swh_rejection_flags'] = xarray.DataArray(swh_rejection_flags,dims='time')
    return ds

# def replace_osw_position_by_rvl(ds_wv):
#     """
#
#     :param ds_wv:
#     :return:
#     """
#     if 'rvlLon' not in ds_wv:
#         logging.warning('file corrupted there is no rvl grid-> exit(-3)')
#         pdb.set_trace()
#         sys.exit(-3)
#     if (abs(ds_wv['oswLon'].values)>180).any() or (np.isfinite(ds_wv['oswLon'].values)==False).any():
#         ds_wv['oswLon'].values = ds_wv['rvlLon'].values
#         logging.info('found %s invalid longitudes in osw I replace by rvl component',abs(ds_wv['oswLon'].values).sum())
#     if (abs(ds_wv['oswLat'].values)>90).any() or (np.isfinite(ds_wv['oswLat'].values)==False).any():
#         ds_wv['oswLat'].values = ds_wv['rvlLat'].values
#         logging.info('found %s invalid latitudes in osw I replace by rvl component',abs(ds_wv['oswLat'].values).sum())
#     return ds_wv

def write_netcdf_file_xarray ( s1_ocn_wv_ds,filout,cwave_version,model_version ,remove_training_vars=True,safename=None) :
    """
    format output daily file (xarray replace cerbere)
    :args:
        s1_ocn_wv_ds : Xarray.Dataset
        wv (str): wv1 or wv2
        cwave_version (str): here this is the format of the netcdf version (different from standards version and product verison definied by CERSAT / CCI )

        redo: (bool)
    """
    logging.debug('add swh_quality')
    cpt = collections.defaultdict(int)
    qcs = 3*np.ones(len(s1_ocn_wv_ds['oswLon'])).astype(int) # default is good
    # one_std = np.std(s1_ocn_wv_ds['swh_uncertainty'].values)
    # qcs[s1_ocn_wv_ds['swh_uncertainty']<one_std] = 3
    # qcs[(s1_ocn_wv_ds['swh_uncertainty'] >= one_std) & (s1_ocn_wv_ds['swh_uncertainty'] < 2*one_std)] = 2
    # qcs[(s1_ocn_wv_ds['swh_uncertainty'] >= 2*one_std) ] = 1

    # defined default zeros quality flags
    s1_ocn_wv_ds['swh_quality'] = xarray.DataArray(qcs,dims=s1_ocn_wv_ds['swh_uncertainty'].dims,
                                                coords=s1_ocn_wv_ds['swh_uncertainty'].coords)
    #je met a zeros puis plus loin dans le code je met tout les indices encore a zeros comme masked
    flags_mean = np.zeros(len(s1_ocn_wv_ds['oswLon'])).astype('O')#*-32768 #pour pouvoir ajouter des bit 2**64 jai limpression que je suis oblige
    s1_ocn_wv_ds['swh_rejection_flags']= xarray.DataArray(flags_mean,
                                dims=s1_ocn_wv_ds['swh_uncertainty'].dims,coords=s1_ocn_wv_ds['swh_uncertainty'].coords)
    # apply quality flags
    s1_ocn_wv_ds = apply_outlier_swh_based_on_uncertainty(s1_ocn_wv_ds)
    s1_ocn_wv_ds = apply_nv_mask(s1_ocn_wv_ds)
    s1_ocn_wv_ds = apply_windspeed_mask(s1_ocn_wv_ds)
    s1_ocn_wv_ds = apply_landmask_in_rejection_flag(s1_ocn_wv_ds)
    s1_ocn_wv_ds = apply_nan_mask(s1_ocn_wv_ds)
    s1_ocn_wv_ds = apply_sea_ice_mask_classif(s1_ocn_wv_ds)
    s1_ocn_wv_ds = apply_raincell_mask_classif(s1_ocn_wv_ds)
    s1_ocn_wv_ds = apply_lwa_mask_classif(s1_ocn_wv_ds)
    s1_ocn_wv_ds = apply_biologicalslicks_mask_classif(s1_ocn_wv_ds)
    s1_ocn_wv_ds = apply_icerberg_mask_classif(s1_ocn_wv_ds)
    # drop classification variables
    s1_ocn_wv_ds = s1_ocn_wv_ds.drop(['IB','LWA','RC','SI','BS'])
    # add the ERA05 meteo params
    dsera05,list_ERA05_files_concerned = get_params_at_locations(lons=s1_ocn_wv_ds['oswLon'].values,
                                                lats=s1_ocn_wv_ds['oswLat'].values,dates=s1_ocn_wv_ds['time'].values)
    for vvera in dsera05:
        s1_ocn_wv_ds[vvera] = dsera05[vvera]
    date_start = s1_ocn_wv_ds['time'].values[0].astype('datetime64[Y]').astype(int) + 1970
    ts = (s1_ocn_wv_ds['time'].values[0] - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')
    firstdate_dt = datetime.datetime.utcfromtimestamp(ts)
    logging.info('date_start : %s',date_start)
    #add bathymetry:
    file_bathy = '/home1/datawork/agrouaze/data/bathymetry/GEBCO_2014_1D.nc'
    instbathy = GEBCO14Bathymetry(file_bathy)
    depths = instbathy.get_depth(s1_ocn_wv_ds['oswLon'].values,s1_ocn_wv_ds['oswLat'].values)
    s1_ocn_wv_ds['bathymetry'] = xarray.DataArray(depths,dims=['time'])
    #compute sea ice concentration
    path_arc = os.path.join(arctic_seaice_path.replace('%Y',firstdate_dt.strftime('%Y')),firstdate_dt.strftime('%Y%m%d') + '.nc')
    path_ant = os.path.join(antarctic_seaice_path.replace('%Y',firstdate_dt.strftime('%Y')),firstdate_dt.strftime('%Y%m%d') + '.nc')
    if os.path.exists(path_arc) and os.path.exists(path_ant):
        seaice_conc = get_seaice_concentration(s1_ocn_wv_ds['oswLon'].values,
                                               s1_ocn_wv_ds['oswLat'].values,
                                               path_arc,
                                               path_ant,
                                               method='bilinear')
    else:
        logging.warning('No ice concentration!!')
        seaice_conc = np.ones((len(s1_ocn_wv_ds['oswLon']),))*np.NaN
    s1_ocn_wv_ds['sea_ice_fraction']= xarray.DataArray(seaice_conc/100.,
                                dims=s1_ocn_wv_ds['swh_uncertainty'].dims,coords=s1_ocn_wv_ds['swh_uncertainty'].coords)
    s1_ocn_wv_ds = apply_sea_ice_mask(s1_ocn_wv_ds,seaice_conc)
    s1_ocn_wv_ds['swh_rejection_flags'].values = s1_ocn_wv_ds['swh_rejection_flags'].values.astype(
        'u2')  # to have SHORT in netCDF
    s1_ocn_wv_ds['swh_rejection_flags'].values = np.ma.masked_where(s1_ocn_wv_ds['swh_rejection_flags'].values==0,
                                                                    s1_ocn_wv_ds['swh_rejection_flags'].values,copy=True)
    #add distance to coast
    landmask,distance_to_coast = swiml2sproc_utils_ancillary_landmask.get_landmask(s1_ocn_wv_ds['oswLon'].values,
                                    s1_ocn_wv_ds['oswLat'].values,land_polygon_path,land_raster_path,debug_figures=False)
    s1_ocn_wv_ds['distance_to_coast']= xarray.DataArray(distance_to_coast*1000.,
                                dims=s1_ocn_wv_ds['swh_uncertainty'].dims,coords=s1_ocn_wv_ds['swh_uncertainty'].coords)
    logging.debug('rename')
    s1_ocn_wv_ds  = s1_ocn_wv_ds.rename({'oswLon':'lon','oswLat':'lat','oswWindSpeed':'wind_speed','nrcs':'sigma0',
                                         'incidence_angle':'angle_of_incidence',
                                         'nv':'normalized_variance',
                                         'u10':'wind_speed_model_u',
                                         'v10':'wind_speed_model_v',
                                         'msl':'surface_air_pressure',
                                         't2m':'surface_air_temperature',
                                         'swh_era5':'swh_model'
                                         })
    if remove_training_vars:
        all_vars_not_needed = ['S','cwave','incidence','latlonSARcossin','dxdt','todSAR','oswIncidenceAngle',
                               'oswQualityCrossSpectraRe','satellite','oswK','oswNrcs','oswLandFlag','platformName',
                               'oswQualityCrossSpectraIm','Sdim','cwavedim','dxdtdim','latlondim','incdim',
                               'oswAngularBinSize','oswWavenumberBinSize'
                               ]
        for vvh in all_vars_not_needed:
            if vvh in s1_ocn_wv_ds:
                s1_ocn_wv_ds = s1_ocn_wv_ds.drop(vvh)
    if len(s1_ocn_wv_ds['swh_uncertainty'])>0:

        #add var attributes
        for kk in s1_ocn_wv_ds.keys():
            logging.debug('att start with : %s',kk)
            if s1_ocn_wv_ds[kk].values.dtype=='float32' or s1_ocn_wv_ds[kk].values.dtype=='float64' or s1_ocn_wv_ds[kk].values.dtype=='int':
                # mask nan elements -> add a fillvalue
                masked_vals = np.ma.masked_where(np.isnan(s1_ocn_wv_ds[kk].values),s1_ocn_wv_ds[kk].values,copy=True)
            else:
                logging.info('variable %s doesnt contains numeric values : %s',kk,s1_ocn_wv_ds[kk].values.dtype)
                masked_vals = s1_ocn_wv_ds[kk].values
            if isinstance(masked_vals,np.ndarray):
                masked_vals = np.ma.array(masked_vals) #to have a valid fillvalue in netcdf attribut
                logging.debug('cast du masked : %s',type(masked_vals))
            logging.debug('s1_ocn_wv_ds.coords : %s',s1_ocn_wv_ds.coords)
            logging.debug('dims %s',s1_ocn_wv_ds[kk].dims)
            if len(s1_ocn_wv_ds[kk].dims)==1:
                coords_tmp = {'time':s1_ocn_wv_ds.coords['time']}
            else:
                coords_tmp = s1_ocn_wv_ds.coords
            s1_ocn_wv_ds[kk] = xarray.DataArray(masked_vals,dims=s1_ocn_wv_ds[kk].dims,coords=coords_tmp)
            logging.debug('%s %s ',kk,type(s1_ocn_wv_ds[kk].values))
            if kk in variables_infos:
                if variables_infos[kk][3] != "":
                    s1_ocn_wv_ds[kk].attrs['units'] = variables_infos[kk][3]
                s1_ocn_wv_ds[kk].attrs['long_name'] =variables_infos[kk][4]
                s1_ocn_wv_ds[kk].attrs['standard_name'] = variables_infos[kk][0]
                # if kk in ['platformName']:
                #     pass
                # else:
                    #s1_ocn_wv_ds[kk].attrs['vmin'] =variables_infos[kk][5]
                    #s1_ocn_wv_ds[kk].attrs['vmax'] =variables_infos[kk][6]
                if variables_infos[kk][5]!="":
                    s1_ocn_wv_ds[kk].attrs['valid_range'] = (variables_infos[kk][5],variables_infos[kk][6])
                s1_ocn_wv_ds[kk].attrs['coordinates'] = "lat lon"
                s1_ocn_wv_ds[kk].attrs['authority'] = "CF 1.8"
                if s1_ocn_wv_ds[kk].dtype=='float32' or s1_ocn_wv_ds[kk].dtype=='float64':
                    s1_ocn_wv_ds[kk].encoding['least_significant_digit'] = 3#'3L'
                    cpt['least_significant_digit'] +=1
                    # make sure the fillvalues are 1e+20
                    ind_fv = (s1_ocn_wv_ds[kk].values > 10000000)
                    s1_ocn_wv_ds[kk].values[ind_fv] = 1e+20
                else:
                    logging.debug('dtype for %s is atypik : %s',kk,s1_ocn_wv_ds[kk].dtype)
                    cpt['no_least_significant_digit'] += 1
                logging.debug('s1_ocn_wv_ds[kk].dtyp %s %s',kk,s1_ocn_wv_ds[kk].dtype)
                #if 'fill_value' in dir(s1_ocn_wv_ds[kk].values): #je ne comprend pas pourquoi le type des values nest pas updated
                #    s1_ocn_wv_ds[kk].attrs['_FillValue'] = s1_ocn_wv_ds[kk].values.fill_value
                if kk in ['swh']:
                    s1_ocn_wv_ds[kk].attrs['coverage_content_type'] = "physicalMeasurement"
                    s1_ocn_wv_ds[kk].attrs['ancillary_variables'] = "swh_quality swh_rejection_flags swh_uncertainty"
                    s1_ocn_wv_ds[kk].attrs['band'] = "C"
                elif kk in ['wind_speed']:
                    s1_ocn_wv_ds[kk].attrs['coverage_content_type'] = "physicalMeasurement"
                    s1_ocn_wv_ds[kk].attrs['band'] = "C"
                elif kk =='bathymetry':
                    s1_ocn_wv_ds[kk].attrs['coverage_content_type'] = "auxiliaryInformation"
                    s1_ocn_wv_ds[kk].attrs['institution'] = "IOC/IHO"
                    s1_ocn_wv_ds[kk].attrs['source'] = "The GEBCO_2014 Grid, version 20150318, www.gebco.net,  doi:10.1002/2015EA000107"
                    if 'least_significant_digit' in s1_ocn_wv_ds[kk].attrs:
                        del s1_ocn_wv_ds[kk].attrs['least_significant_digit']
                elif kk in ['angle_of_incidence']:
                    #del s1_ocn_wv_ds[kk].attrs['least_significant_digit']

                    s1_ocn_wv_ds[kk].encoding = {}
                    s1_ocn_wv_ds[kk].encoding['_FillValue'] = 1.e20
                    if 'least_significant_digit' in s1_ocn_wv_ds[kk].attrs :
                        del s1_ocn_wv_ds[kk].attrs['least_significant_digit']
                    logging.info('list attrs inc: %s',s1_ocn_wv_ds[kk].attrs.keys())
                    #s1_ocn_wv_ds[kk].encoding = {'least_significant_digit' : 30} #for test 30 instead of3

                elif kk in 'swh_quality':
                    #s1_ocn_wv_ds[kk].encoding['least_significant_digit'] = '0L'
                    s1_ocn_wv_ds[kk].attrs['flag_values'] = [0,1,2,3]#"0L, 1L, 2L, 3L"
                    s1_ocn_wv_ds[kk].attrs['flag_meanings'] = "undefined bad acceptable good"
                    s1_ocn_wv_ds[kk].attrs['coverage_content_type'] = "qualityInformation"
                    s1_ocn_wv_ds[kk].attrs['band'] = "C"
                elif kk in 'sea_ice_fraction':
                    s1_ocn_wv_ds[kk].attrs['coverage_content_type'] = "auxiliaryInformation"
                    s1_ocn_wv_ds[kk].attrs['institution'] = "ESA"
                    s1_ocn_wv_ds[kk].attrs['source'] = "CCI Sea Ice"
                    s1_ocn_wv_ds[kk].encoding['least_significant_digit'] = 2
                    s1_ocn_wv_ds[kk].attrs['source_files'] = [os.path.basename(path_arc),os.path.basename(path_ant)]
                elif kk in ['surface_air_temperature','wind_speed_model_u','wind_speed_model_u','surface_air_pressure','swh_model']:
                    s1_ocn_wv_ds[kk].attrs['source_files'] = [os.path.basename(ffer) for ffer in list_ERA05_files_concerned]
                    s1_ocn_wv_ds[kk].attrs['source'] = "Copernicus ERA5 Reanalysis by ECMWF"
                elif kk in 'swh_rejection_flags':
                    #s1_ocn_wv_ds[kk].encoding['least_significant_digit'] = '0L'
                    s1_ocn_wv_ds[kk].attrs['flag_masks'] = [1,2,4,8,16,32,64,128,256,512,1024,2048,4096] #"1L, 2L, 4L, 8L, 16L, 32L, 64L, 128L"
                    #s1_ocn_wv_ds[kk].attrs['flag_meanings'] = "nb_of_valid_swh_too_low swh_validity not_water sea_ice sigma0_validity waveform_validity swh_rms_outlier swh_outlier"
                    s1_ocn_wv_ds[kk].attrs['flag_meanings'] = "nv_greater_than_max_nv swh_outlier invalid_value wind_below_2_m_per_sec\
                     not_water sea_ice_classification biological_slick_classification rain_cell_classification\
                      iceberg_bright_target_classification low_wind_area_classification"
                    s1_ocn_wv_ds[kk].attrs['coverage_content_type'] = "qualityInformation"
                    s1_ocn_wv_ds[kk].attrs['band'] = "C"
                    #del s1_ocn_wv_ds[kk].attrs['units']
                    #del s1_ocn_wv_ds[kk].attrs['valid_range']
                    #s1_ocn_wv_ds[kk].attrs['_FillValue'] = -32768
                elif kk in 'distance_to_coast':
                    s1_ocn_wv_ds[kk].attrs['coverage_content_type'] = "auxiliaryInformation"
                    s1_ocn_wv_ds[kk].encoding['_FillValue'] = 1.e20
                    s1_ocn_wv_ds[kk].attrs['institution'] = 'OpenStreetMap'
                    s1_ocn_wv_ds[kk].attrs['source'] = "Openstreetmap v2016 : using hybrid method raster/polygons"
                    del s1_ocn_wv_ds[kk].attrs['standard_name']
                elif kk in 'sigma0':
                    s1_ocn_wv_ds[kk].attrs['coverage_content_type'] = "physicalMeasurement"
                    if model_version=='exp1':
                        s1_ocn_wv_ds[kk].attrs['long name'] = "sigma0 denoised (Normalized Radar Cross Section)"
                        #for classical predictions from L2, this is the oswNrcs value that is used and in June 2021 still not denoised
                else:
                    logging.info('nothing special for var: %s',kk)
                logging.debug('attrb for %s added: %s',kk,s1_ocn_wv_ds[kk].attrs)
            #elif kk in ['time']:

            else:
                logging.debug('no att for %s',kk)
        #time configuration
        s1_ocn_wv_ds['time'].attrs[
            'long_name'] = "start time of the WV acquisition (lasts less than 3 seconds for one image)"
        s1_ocn_wv_ds['time'].encoding['units'] = variables_infos['time'][3]
        s1_ocn_wv_ds['time'].encoding['_FillValue'] = -2147483648.
        s1_ocn_wv_ds['time'].attrs["axis"] = "T"
        s1_ocn_wv_ds['time'].attrs["standard_name"] = "time"
        s1_ocn_wv_ds['time'].attrs["long_name"] = "measurement time"
        #longitude
        s1_ocn_wv_ds['lon'].attrs["axis"] = "X"
        s1_ocn_wv_ds['lon'].encoding["_FillValue"] = 1.e+20
        #latitude
        s1_ocn_wv_ds['lat'].attrs["axis"] = "Y"
        s1_ocn_wv_ds['lat'].encoding["_FillValue"] = 1.e+20
        lami = s1_ocn_wv_ds['lat'].min().values
        lama = s1_ocn_wv_ds['lat'].max().values
        lomi = s1_ocn_wv_ds['lon'].min().values
        loma = s1_ocn_wv_ds['lon'].max().values
        # https://gis.stackexchange.com/questions/237116/sentinel-1-relative-orbit#:~:text=You%20actually%20can%20find%20the,%2D%2073%2C%20175)%20%2B%201
        #Sentinel-1A Relative Orbit Number = mod (Absolute Orbit Number orbit - 73, 175) + 1,
        #Sentinel-1B Relative Orbit Number = mod (Absolute Orbit Number orbit - 27, 175) + 1
        absOrbitNum = int(os.path.basename(safename).split('_')[7])
        if 'S1A' in safename:
            relOrNum = (absOrbitNum-73)%175+1
        elif 'S1B' in safename:
            relOrNum = (absOrbitNum - 27) % 175 + 1
        else:
            raise Exception('unreferenced platform S1 (relative orbit number computation')
        globatt = {
            'institution' : 'University of Hawaii , Laboratory of Physical and Spatial Oceanography  Institut Français pour la Recherche et l Exploitation de la MER, European Space Agency',
            'institution_abbreviation' : 'UH, Ifremer-LOPS, ESA',
            'publisher_name' : "cersat",
            'publisher_url' : "cersat.ifremer.fr",
            'publisher_email' : "cersat@ifremer.fr",
            'publisher_institution' : 'Ifremer / Cersat',
            #'PIs' : 'Justin Stopa, Alexis Mouche',
            #'reference paper' : 'Stopa, Mouche JGR oceans 2017 https://doi.org/10.1002/2016JC012364',
            #'reference paper': 'Quach et al 2020 https://authors.library.caltech.edu/104562/1/09143500.pdf',
            #'incidence_angle' : wv,
            #'version of NN model' : cwave_version,
            'time_coverage_start' :pd.to_datetime(str( s1_ocn_wv_ds['time'].min().values)).strftime('%Y-%m-%dT%H:%M:%SZ'), #"2017-01-01T00:28:08Z" ;             #new att like cerbere
            'time_coverage_end' : pd.to_datetime(str(s1_ocn_wv_ds['time'].max().values)).strftime('%Y-%m-%dT%H:%M:%SZ'),
            'Conventions' : "CF-1.7, ACDD-1.3, ISO 8601" ,
            'title' : "ESA CCI Sea State L2P derived from Sentinel-1 Wave Mode, using Quach et al., 2020", #"ESA CCI Sea State L2 ESA OCN from Sentinel-1",
            'id' : "ESACCI-SEASTATE-L2P-SWH--Sentinel-%s-WV-QUACH2020"%os.path.basename(filout).split('-')[5], #"ESACCI-SEASTATE-L2P-SWH-Sentinel1" ,
            #'institution = "Institut Francais de Recherche pour l\'Exploitation de la mer / CERSAT, European Space Agency" ;
            'source' : "CCI Sea State Sentinel-1 Quach et al 2020 Statistical Hs Processor",
            'history' : "%s - Creation"%datetime.datetime.today().strftime('%Y-%m-%dT%H:%M:%SZ'),
            'references' : ["CCI Sea State Product Specification Document (PSD), v2","Quach et al 2020 https://authors.library.caltech.edu/104562/1/09143500.pdf"],
            'product_version' : "1.0",
            'summary' : "This dataset contains significant wave height measurements from Sentinel-1 SAR, using Quach 2020" ,
            'keywords' : ("Oceans > Ocean Waves > Significant Wave Height", "Oceans > Ocean Waves > Sea State"),
            'keywords_vocabulary' : "NASA Global Change Master Directory (GCMD) Science Keywords",
            'naming_authority' : "fr.ifremer.cersat" ,
            'cdm_data_type' : "trajectory",
            'featureType' : "trajectory" ,
            'comment' : "These data were produced at Ifremer as part of the ESA SST CCI project.",
            'creator_name' : "Justin Stopa, Alexis Mouche" ,
            'creator_url' : ["https://www.soest.hawaii.edu/ore/","https://www.umr-lops.fr/"] ,
            'creator_email' : ["stopa@hawaii.edu","alexis.mouche@ifremer.fr"] ,
            'creator_institution' : ["Ifremer / LOPS","University of Hawaii / SOEST / Ocean and Resources Engineering Dpt."] ,
            'project' : "Climate Change Initiative - European Space Agency" ,
            'geospatial_lat_min' : lami,
            'geospatial_lat_max' : lama ,
            'geospatial_lat_units' : "degree_north" ,
            'geospatial_lon_min' : lomi,
            'geospatial_lon_max' : loma ,
            'geospatial_lon_units' : "degree_east" ,
            'geospatial_bounds':"POLYGON((%s %s,%s %s,%s %s,%s %s,%s %s))"%(lomi,lami,lomi,lama,loma,lama,loma,lami,lomi,lami),
            'standard_name_vocabulary' : "NetCDF Climate and Forecast (CF) Metadata Convention version 1.8" ,
            'license' : "ESA CCI Data Policy: free and open access" ,
            'platform' : "Sentinel-1 %s"%os.path.basename(filout).split('-')[5][-1] ,
            'platform_type' : "low earth orbit satellite" ,
            'platform_vocabulary' : "CEOS" ,
            'instrument' : "C-band SAR" ,
            'instrument_type' : "SAR (Synthetic Aperture Radar)" ,
            'instrument_vocabulary' : "CEOS" ,
            "acquisition_mode": "wave mode",
            'spatial_resolution' : "20 km" ,
            'cycle_number':'',
            'relative_pass_number':'%s'%relOrNum,
            'equator_crossing_time':'',
            'equator_crossing_longitude':'',
            'netcdf_version_id' : "%s"%netCDF4.__version__,
            'acknowledgement' : "Please acknowledge the use of these data with the following statement: these data were obtained from the ESA CCI Sea State project" ,
            'format_version' : "Data Standards v2.1" ,
            'processing_level' : "L2P" ,
            'scientific_support_contact' : "stopa@hawaii.edu" ,
            'technical_support_contact' : "cersat@ifremer.fr" ,
            'key_variables' : "swh" ,
            'date_created' : datetime.datetime.today().strftime('%Y-%m-%dT%H:%M:%SZ'),
            'date_modified' : datetime.datetime.today().strftime('%Y-%m-%dT%H:%M:%SZ'),
            'track_id':"%s"%uuid.uuid4(),
            'band' : "C" ,
            'source_version' : "Quach 2020 / %s"%cwave_version ,
            #'source_product': safename,
            'input' : "Level-2 ESA WV OCN products: %s"%safename ,
            'Metadata_Conventions' : "Climate and Forecast (CF) 1.7, Attribute Convention for Data Discovery (ACDD) 1.3" ,
            'geospatial_vertical_units' : "meters above mean sea level" ,
            'geospatial_vertical_positive' : "up"

        }
        for kki in globatt:
            s1_ocn_wv_ds.attrs[kki] = globatt[kki]
        #float_fillvalue = netCDF4.default_fillvals['f8']
        float_fillvalue = 1.e20
        short_fillvalue = netCDF4.default_fillvals['i2']
        encoding = {'lat' : {'zlib' : False,'_FillValue' : 1.e+20},
                    'lon' : {'zlib' : False,'_FillValue' : 1.e+20},
                    'swh_rejection_flags' : {'dtype':'u2'}, #'_FillValue' : -32768,
                    #'incidence_angle' : {'_FillValue' : float_fillvalue},

                    'wind_speed' : {'_FillValue' : float_fillvalue},
                    'sigma0' : {'_FillValue' : float_fillvalue},
                    'normalized_variance' : {'_FillValue' : float_fillvalue},
                    'heading' : {'_FillValue' : float_fillvalue},
                    'swh' : {'_FillValue' : float_fillvalue},
                    'swh_uncertainty' : {'_FillValue' : float_fillvalue},
                    'swh_quality' : {'_FillValue' : 0,'dtype':np.ubyte},
                    'distance_to_coast' : {'_FillValue' : float_fillvalue,'dtype':np.float32},
                    'sea_ice_fraction':{'_FillValue' : float_fillvalue},
                    'surface_air_pressure':{'_FillValue' : float_fillvalue},
                    'wind_speed_model_u' : {'_FillValue' : float_fillvalue},
                    'wind_speed_model_v' : {'_FillValue' : float_fillvalue},
                    'surface_air_temperature' : {'_FillValue' : float_fillvalue},
                    'swh_model' : {'_FillValue' : float_fillvalue},
                    'bathymetry':{'_FillValue':short_fillvalue,'dtype':'i2'},
                    }
        s1_ocn_wv_ds.to_netcdf(filout,encoding=encoding)
        logging.info('done! output : %s',filout)
        status = 'written'
    else :
        logging.info('there is no hs total SAR for this day and this satellite %s',filout)
        status = 'nodata'
    logging.info('stats : %s',cpt)
    return status


def process_one_orbit_v2 ( safename,model,args ) :
    """
    method that take a OCN SAFE, do Hs predictions using NN model and then output a new nc file compliant
    with CCI sea state requirement
    args:
        safename : str (basename L2 S1 WV)
    """
    logging.info('treat safe: %s',safename)
    final_df = None
    sat = safename[0 :3]
    if args.outputdir is None :
        #outputdir = os.path.join(OUTDIR,args.cwave_version,sat + '_' + args.wv)
        outputdir = os.path.join(OUTDIR,args.cwave_version,sat)
    else :
        #outputdir = os.path.join(args.outputdir,args.cwave_version,sat + '_' + args.wv)
        outputdir = os.path.join(args.outputdir,args.cwave_version,sat)
    datestart_from_safe = datetime.datetime.strptime(os.path.basename(safename).split('_')[5],'%Y%m%dT%H%M%S')
    # filout = os.path.join(outputdir,datestart_from_safe.strftime('%Y'),datestart_from_safe.strftime('%j'),
    #                       safename.replace('.SAFE','')+ '_level2_LOPS_SWH_SAR_%s.nc' % args.cwave_version)
    # version DLR
    filout = os.path.join(outputdir,datestart_from_safe.strftime('%Y'),datestart_from_safe.strftime('%j'),
                          'ESACCI-SEASTATE-L2P-SWH-Sentinel-%s-%s-QUACH2020_%s-fv%s.nc'%(sat[1:],
                                        os.path.basename(safename).split('_')[5], args.cwave_version,FILE_VERSION))
    logging.info('final path: %s',filout)
    dira = os.path.dirname(filout)

    if os.path.exists(dira) is False :
        os.makedirs(dira,0o0755)
        logging.info('make dir %s',dira)
    if os.path.exists(filout) and args.redo is True :
        os.remove(filout)
    if os.path.exists(filout) and args.redo is False :
        status = 'already_in'
        logging.info('output file already exists')
    else :
        logging.info('outputdir = %s',outputdir)
        final_ds = read_infos_from_WV_ifremer_archive_v3(safename,model=model,model_version=args.model_version,dev=args.dev)
        if final_ds is not None:
            logging.debug('%s',final_ds.keys())
            logging.debug('%s',final_ds.count())
            logging.info('write the final netCDF')
            status = write_netcdf_file_xarray(final_ds,filout,cwave_version=args.cwave_version,model_version=args.model_version,safename=os.path.basename(safename))
        else:
            logging.info('No WV OCN data for safe : %s ',safename)
            status = 'nodata'
    return status,final_df


if __name__ == '__main__' :
    tinit = time.time()
    root = logging.getLogger()
    if root.handlers :
        for handler in root.handlers :
            root.removeHandler(handler)
    import argparse

    parser = argparse.ArgumentParser(description='hs_sar_product')
    parser.add_argument('--verbose',action='store_true',default=False)
    parser.add_argument('--outputdir',default=None,help='folder where the data will be written [optional]',
                        required=False)
    parser.add_argument('--safename',required=True,help=' L2 .SAFE to process',type=str)
    #parser.add_argument('--wv',required=True,help='wv1 or wv2...',type=str)
    parser.add_argument('--redo',action='store_true',default=False,help='redo existing files nc')
    parser.add_argument('--cwave-version',required=True,help='example  v1.2')
    parser.add_argument('--model_version',required=False,
            help='[optional , default is the model trained by P. Sadowsky in feb 2021], possible model exp1 ...')
    parser.add_argument('--dev',action='store_true',default=False,help='dev/test mode only 2 wv measu treated in a day')
    args = parser.parse_args()
    fmt = '%(asctime)s %(levelname)s %(filename)s(%(lineno)d) %(message)s'
    if args.verbose :
        logging.basicConfig(level=logging.DEBUG,format=fmt,
                            datefmt='%d/%m/%Y %H:%M:%S')
    else :
        logging.basicConfig(level=logging.INFO,format=fmt,
                            datefmt='%d/%m/%Y %H:%M:%S')
    #     datedt = datetime.datetime(2017,1,25,)
    if args.model_version is None:
        model = load_quach2020_model_v2() #heteroskedastik 2017 for format validation before final model release from Hawaii team
    elif args.model_version=='exp1':
        logging.info('load model NN from experiment #1')
        model = load_quach2020_model_exp1()
    else:
        raise Exception('unknown model')
    cptu = collections.defaultdict(int)

    t0 = time.time()
    if args.safename[-1] == '/':
        inputsafepath = args.safename[0:-1]
    else:
        inputsafepath = args.safename
    status,final_df = process_one_orbit_v2(os.path.basename(inputsafepath),model,args)
    cptu[status] += 1
    elapsed = datetime.timedelta(seconds=(time.time() - t0))
    logging.info('time to write one file = %s',elapsed)
    logging.info('counter = %s',cptu)
    logging.info('script main finished in %1.1f seconds',time.time()-tinit)
    logging.info('peak memory usage: %s kilobytes',resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)
